{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三章 推薦システムの高速化\n",
    "\n",
    "## モデルベースの推薦システム\n",
    "\n",
    "第二章の手法は**メモリベース**と呼ばれる方法でした。今回は**モデルベース**を試します。\n",
    "\n",
    "## 分類モデルを用いた内容ベースフィルタリング\n",
    "\n",
    "さっきと一緒じゃね？\n",
    "分類モデルとは**すでにラベル付されたデータを元に統計的な手法を用いて、ラベル付いていないデータに特定のラベルを与える手法です。**\n",
    "\n",
    "分類モデルの種類には\n",
    "\n",
    "* ナイーブベイズ（NB）\n",
    "* ロジスティック回帰（LR）\n",
    "* サポートベクターマシーン（SVM）\n",
    "\n",
    "今回はナイーブベイズを使います。\n",
    "本にはナイーブベイズは簡単に組めるけどLRとSVMに対し、制度が劣るらしいです。\n",
    "\n",
    "そんなデメリットに対しメリットとして\n",
    "\n",
    "* モデルをつくるための計算時間が短い\n",
    "* 新しいデータに対してモデルの更新がしやすい\n",
    "* モデルの中身が見やすく、手動でのチューニングが容易\n",
    "\n",
    "ナイーブベイズの中身についての説明\n",
    "\n",
    "$P(c|X) = \\dfrac{P(c,X)}{P(X)} = \\dfrac{P(X|c)P(c)}{P(X)}$\n",
    "\n",
    "ある事象Xが発生したとき、それが暮らすcに属する確率$P(c|X)$を求めることでクラス分類を行います。\n",
    "\n",
    "もっとよく知りたい方はこちら\n",
    "\n",
    "* X=ガンダム、Y={連邦軍, ジオン軍（連邦軍じゃない）}\n",
    "* X=RX78-2、Y={ガンダム,ザク,ジオング}\n",
    "* X=赤いMS、Y={シャアザク,ガンタンク,ズゴッグ}\n",
    "\n",
    "のように、Xには何らかの起こった事象や入力データを、Yにはそこから推論したい事柄などを当てはめるわけです。\n",
    "※尤度 ≒ 条件付き確率\n",
    "\n",
    "http://qiita.com/aflc/items/13fe52243c35d3b678b0\n",
    "※最初はわかりやすいが後半数式ばっかで涙目\n",
    "\n",
    "では、事象Xがn次元空間上のベクトル表現として$X=(x_1,x_2,\\dots,x_n)$と表せるときに、特徴空間上で独立であると仮定すれば、$P(X|c)$は次のように表せます。\n",
    "\n",
    "$P(X|c) = P(x_1,x_2,\\dots,x_n|c) = P(x_1|c)P(x_2|c) \\cdots P(x_n|c)$\n",
    "\n",
    "ここである事象Xに対して、もっとも$P(c|X)$が大きくなるクラスcを与える問題を考えれば$P(X)$は共通になります。\n",
    "\n",
    "$P(c)(x_1|c)(x_2|c) \\cdots P(x_n|c)$\n",
    "\n",
    "が、最大になるcを与えればいいということがわかります（わかります？）\n",
    "\n",
    "このとき一般に$P(x_i|c)$は微小になりアンダフローを起こしやすいため、次のように対数表現で表します。\n",
    "\n",
    "$log P(c) + log P(x_1|c) + log P (x_2|c) + \\cdots + log P(x_n|c)$\n",
    "\n",
    "このとき最大化するクラスcを求めます。これがナイーブベイズ分類器です。このナイーブベイズ分類器を用いた文章推薦システムを考えます。\n",
    "\n",
    "今回はユーザーにニュースなどの文章を推薦するシステムで、ユーザーはその中で読みたくなかった文章についてはシステムにフィードバッグできるしくみがあるとします。ユーザーが読んだ文章のクラスを正解クラス、ユーザーがよみたくなかった文章のクラスを不正解クラスとし、ナイーブベイズ分類器に酔って正解クラスに分類されるものが推薦される文章です。\n",
    "\n",
    "まず文章をベクトル表現します。文章を単語の集合として表現することを考えます。この表現方法を**Bag of Words**と呼びます。\n",
    "\n",
    "\n",
    "ある文章dが単語$w_1,\\cdots,w_N$で表現されるとき、確率は次のように計算できます。\n",
    "\n",
    "$P(w_i|c) = \\dfrac{クラスの文章で単語w_iを含む文章数}{クラスの文章数}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ナイーブベイズはscikit-learnでできるんだって！すげぇ！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris() # サンプルデータセットの\"iris（花？）\"を取得\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB() # ガウシアンなイーブイズ分類器をセット\n",
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data) # 入力データ iris.data、出力データ iris.targetとして学習（fit）してテストデータを投げる（predict)\n",
    "\n",
    "print(y_pred) # んで、結果とね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(iris.target != y_pred).sum() # 予測したデータ（学習データ）と入力データを比較したら6個間違いがある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次元削減を用いた協調フィルタリング\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import top_modeling as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.0192881   2.9479985   4.44135127  0.99521866]\n",
      " [ 3.98040699  2.34580851  3.70550433  0.99497778]\n",
      " [ 1.07584801  0.81719661  5.21117303  4.99113573]\n",
      " [ 0.96223933  0.71246821  4.23721761  3.98882969]\n",
      " [ 1.84884681  1.22872866  4.91890374  4.04926914]]\n"
     ]
    }
   ],
   "source": [
    "R = numpy.array([\n",
    "  [5, 3, 0, 1],\n",
    "  [4, 0, 0, 1],\n",
    "  [1, 1, 0, 5],\n",
    "  [1, 0, 0, 4],\n",
    "  [0, 1, 5, 4]\n",
    "])\n",
    "\n",
    "nP, nQ = tm.matrix_factorization(R, 2)\n",
    "nR = numpy.dot(nP.T, nQ)\n",
    "\n",
    "print(nR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
